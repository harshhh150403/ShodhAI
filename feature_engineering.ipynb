{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Feature Engineering & Data Cleaning\n",
                "\n",
                "**Goal**: Prepare the LendingClub dataset for Deep Learning and Offline Reinforcement Learning models.\n",
                "\n",
                "**Key Steps**:\n",
                "1.  **Target Definition**: 'Fully Paid' (0) vs. 'Charged Off'/'Default' (1).\n",
                "2.  **Leakage Removal**: Remove future-looking variables (e.g., specific payments, recoveries).\n",
                "3.  **Feature Engineering**: Create ratios, handle dates, process categorical risk factors.\n",
                "4.  **Cleaning & Preprocessing**: Handle missing values, encode categoricals, and scale features.\n",
                "5.  **Output**: Save processed train/test sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "# Configuration\n",
                "DATA_PATH = Path(\"data/accepted_2007_to_2018Q4.csv\")\n",
                "PROCESSED_DATA_DIR = Path(\"data/processed\")\n",
                "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Set to 200000 (approx 10% of data) to fit comfortably within 16GB RAM.\n",
                "# Set to None ONLY if you have >32GB RAM available.\n",
                "SAMPLE_SIZE = False\n",
                "\n",
                "pd.set_option(\"display.max_columns\", None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading dataset...\n",
                        "Loaded FULL dataset.\n",
                        "Initial shape: (2260701, 151)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading dataset...\")\n",
                "if SAMPLE_SIZE:\n",
                "    # Load a chunk for efficiency\n",
                "    df_iter = pd.read_csv(DATA_PATH, low_memory=False, chunksize=SAMPLE_SIZE)\n",
                "    df = next(df_iter)\n",
                "    print(f\"Loaded SAMPLE of {SAMPLE_SIZE} rows.\")\n",
                "else:\n",
                "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
                "    print(f\"Loaded FULL dataset.\")\n",
                "\n",
                "print(f\"Initial shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Target Definition\n",
                "We only consider completed loans: 'Fully Paid' and 'Charged Off' (including 'Default').\n",
                "We drop 'Current' loans as their final status is unknown."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Shape after filtering target: (1345350, 152)\n",
                        "Target Distribution:\n",
                        "target\n",
                        "0    0.80035\n",
                        "1    0.19965\n",
                        "Name: proportion, dtype: float64\n"
                    ]
                }
            ],
            "source": [
                "target_status = ['Fully Paid', 'Charged Off', 'Default']\n",
                "df = df[df['loan_status'].isin(target_status)].copy()\n",
                "\n",
                "# Map to Binary Target: 0 = Fully Paid, 1 = Default/Charged Off\n",
                "df['target'] = df['loan_status'].apply(lambda x: 0 if x == 'Fully Paid' else 1)\n",
                "\n",
                "print(f\"Shape after filtering target: {df.shape}\")\n",
                "print(\"Target Distribution:\")\n",
                "print(df['target'].value_counts(normalize=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Cleaning: Remove Leakage & Irrelevant Features\n",
                "We diligently remove features that are not available at the time of application ($t=0$). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dropped 45 columns. New shape: (1345350, 107)\n"
                    ]
                }
            ],
            "source": [
                "leakage_cols = [\n",
                "    # Funding info (post-approval)\n",
                "    'funded_amnt', 'funded_amnt_inv',\n",
                "    \n",
                "    # Payment tracking (future info)\n",
                "    'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee',\n",
                "    'recoveries', 'collection_recovery_fee',\n",
                "    'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d',\n",
                "    'last_credit_pull_d',\n",
                "    \n",
                "    # Current Status (implies non-default)\n",
                "    'out_prncp', 'out_prncp_inv',\n",
                "    \n",
                "    # Settlement & Hardship (only for distressed loans)\n",
                "    'debt_settlement_flag', 'debt_settlement_flag_date', \n",
                "    'settlement_status', 'settlement_date', 'settlement_amount', 'settlement_percentage', 'settlement_term',\n",
                "    'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'hardship_length', \n",
                "    'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date', 'hardship_loan_status', \n",
                "    'hardship_dpd', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount',\n",
                "    'orig_projected_additional_accrued_interest',\n",
                "    'chargeoff_within_12_mths',\n",
                "    'collections_12_mths_ex_med'\n",
                "]\n",
                "\n",
                "irrelevant_cols = [\n",
                "    'id', 'member_id', 'url', 'desc',\n",
                "    'title', 'zip_code', 'policy_code',\n",
                "    'emp_title' # High cardinality feature that causes OHE explosion\n",
                "]\n",
                "\n",
                "cols_to_drop = leakage_cols + irrelevant_cols\n",
                "# Only drop columns that actually exist in the dataframe\n",
                "cols_to_drop = [c for c in cols_to_drop if c in df.columns]\n",
                "\n",
                "df.drop(columns=cols_to_drop, inplace=True)\n",
                "print(f\"Dropped {len(cols_to_drop)} columns. New shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Engineering\n",
                "Creating new features and transforming existing ones for better model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_17356\\920015685.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
                        "  df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
                        "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_17356\\920015685.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
                        "  df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Feature engineering complete.\n"
                    ]
                }
            ],
            "source": [
                "# 1. Term to Numeric\n",
                "df['term'] = df['term'].str.replace(' months', '').astype(float)\n",
                "\n",
                "# 2. Emp Length to Numeric\n",
                "emp_map = {\n",
                "    '< 1 year': 0,\n",
                "    '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4, '5 years': 5,\n",
                "    '6 years': 6, '7 years': 7, '8 years': 8, '9 years': 9, '10+ years': 10\n",
                "}\n",
                "df['emp_length'] = df['emp_length'].map(emp_map)\n",
                "\n",
                "# 3. Date Features\n",
                "# Ensure datetime\n",
                "df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
                "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
                "\n",
                "# Credit History Length (in years)\n",
                "df['credit_history_years'] = (df['issue_d'] - df['earliest_cr_line']).dt.days / 365.25\n",
                "# Handle potential negative values or errors if issue_d < earliest_cr_line (rare data errors)\n",
                "df['credit_history_years'] = df['credit_history_years'].clip(lower=0)\n",
                "\n",
                "# 4. FICO Score Average\n",
                "df['fico_score'] = 0.5 * (df['fico_range_low'] + df['fico_range_high'])\n",
                "df.drop(columns=['fico_range_low', 'fico_range_high'], inplace=True)\n",
                "\n",
                "# 5. New Risk Indicators\n",
                "df['loan_to_income'] = df['loan_amnt'] / df['annual_inc']\n",
                "df['revol_bal_to_income'] = df['revol_bal'] / df['annual_inc']\n",
                "df['has_delinquency'] = (df['delinq_2yrs'] > 0).astype(int)\n",
                "\n",
                "# Handle inf values created by division by zero if annual_inc is 0\n",
                "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "\n",
                "print(\"Feature engineering complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Handling Missing Values\n",
                "Dropping high-missingness columns (except critical ones) and imputing the rest."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dropped 37 columns due to missingness > 50%.\n"
                    ]
                }
            ],
            "source": [
                "# Calculate missing %\n",
                "missing_pct = df.isnull().mean() * 100\n",
                "\n",
                "# Drop columns with > 50% missing, conserving important ones if needed\n",
                "# (User feedback: 'emp_length' is predictive, we kept it and mapped it, now we will impute it)\n",
                "drop_threshold = 50\n",
                "cols_to_drop_missing = missing_pct[missing_pct > drop_threshold].index.tolist()\n",
                "\n",
                "# Explicitly KEEP emp_length even if it was > 50% (it's around 6% usually, but good to be safe)\n",
                "if 'emp_length' in cols_to_drop_missing:\n",
                "    cols_to_drop_missing.remove('emp_length')\n",
                "\n",
                "df.drop(columns=cols_to_drop_missing, inplace=True)\n",
                "print(f\"Dropped {len(cols_to_drop_missing)} columns due to missingness > {drop_threshold}%.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train/Test Split (Temporal awareness)\n",
                "We use `issue_d` to split, then drop it from the feature set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train shape: (1076280, 73)\n",
                        "Test shape: (269070, 73)\n"
                    ]
                }
            ],
            "source": [
                "# Sort by date to simulate real-world 'past predicting future' scenario if strictly needed,\n",
                "# or just random split if we assume stationarity. \n",
                "# Included `issue_d` preservation plan: Keep relevant for analysis.\n",
                "# For this baseline, we'll do a standard random split but keep date for analysis if needed.\n",
                "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
                "\n",
                "print(f\"Train shape: {train_df.shape}\")\n",
                "print(f\"Test shape: {test_df.shape}\")\n",
                "\n",
                "# Drop issue_d and earliest_cr_line from inputs\n",
                "date_cols = ['issue_d', 'earliest_cr_line']\n",
                "train_df = train_df.drop(columns=date_cols)\n",
                "test_df = test_df.drop(columns=date_cols)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Preprocessing Pipeline\n",
                "- **Imputation**: Median for numeric, 'Unknown' for categorical.\n",
                "- **Encoding**: Ordinal for 'grade', One-Hot for others.\n",
                "- **Scaling**: Standard Scaler."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Numeric features: 58\n",
                        "Categorical features: 9\n",
                        "Ordinal features: 1\n"
                    ]
                }
            ],
            "source": [
                "# Define Features\n",
                "y_train = train_df['target']\n",
                "y_test = test_df['target']\n",
                "X_train = train_df.drop(columns=['target', 'loan_status'])\n",
                "X_test = test_df.drop(columns=['target', 'loan_status'])\n",
                "\n",
                "# Identify column types\n",
                "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
                "\n",
                "# Specific handling for Grade (Ordinal)\n",
                "if 'grade' in categorical_features:\n",
                "    categorical_features.remove('grade')\n",
                "    ordinal_features = ['grade']\n",
                "else:\n",
                "    ordinal_features = []\n",
                "\n",
                "print(f\"Numeric features: {len(numeric_features)}\")\n",
                "print(f\"Categorical features: {len(categorical_features)}\")\n",
                "print(f\"Ordinal features: {len(ordinal_features)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fitting preprocessor...\n",
                        "Processed Train shape: (1076280, 175)\n"
                    ]
                }
            ],
            "source": [
                "# Define Transformers\n",
                "\n",
                "# Numeric: Impute Median -> Scale\n",
                "numeric_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "# Categorical High Card/Nominal: Impute 'Unknown' -> OneHot\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
                "])\n",
                "\n",
                "# Ordinal (Grade): Impute -> Ordinal Encode\n",
                "grade_order = [['A', 'B', 'C', 'D', 'E', 'F', 'G']]\n",
                "ordinal_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('ordinal', OrdinalEncoder(categories=grade_order))\n",
                "])\n",
                "\n",
                "# Combine\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numeric_transformer, numeric_features),\n",
                "        ('cat', categorical_transformer, categorical_features),\n",
                "        ('ord', ordinal_transformer, ordinal_features)\n",
                "    ],\n",
                "    verbose_feature_names_out=False\n",
                ")\n",
                "\n",
                "# Fit & Transform\n",
                "# Note: We fit on Train, transform both\n",
                "print(\"Fitting preprocessor...\")\n",
                "X_train_processed = preprocessor.fit_transform(X_train)\n",
                "X_test_processed = preprocessor.transform(X_test)\n",
                "\n",
                "# Get feature names back\n",
                "try:\n",
                "    feature_names = preprocessor.get_feature_names_out()\n",
                "except AttributeError:\n",
                "    # Fallback for older sklearn versions if needed, though usually not an issue with updated envs\n",
                "    feature_names = numeric_features + list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)) + ordinal_features\n",
                "\n",
                "# Convert back to DataFrame\n",
                "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
                "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
                "\n",
                "print(f\"Processed Train shape: {X_train_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saving processed data...\n",
                        "Data saved to data/processed/\n"
                    ]
                }
            ],
            "source": [
                "print(\"Saving processed data...\")\n",
                "# Save compressed to save space\n",
                "X_train_df.to_parquet(PROCESSED_DATA_DIR / \"X_train.parquet\")\n",
                "X_test_df.to_parquet(PROCESSED_DATA_DIR / \"X_test.parquet\")\n",
                "y_train.to_frame().to_parquet(PROCESSED_DATA_DIR / \"y_train.parquet\")\n",
                "y_test.to_frame().to_parquet(PROCESSED_DATA_DIR / \"y_test.parquet\")\n",
                "\n",
                "print(\"Data saved to data/processed/\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
